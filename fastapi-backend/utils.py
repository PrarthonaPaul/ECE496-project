import os
import cohere
from collections import defaultdict
from datasets import Dataset
from dotenv import load_dotenv
import fitz  # PyMuPDF
import pandas as pd
from transformers import RobertaTokenizer
import torch

def pdf_to_text(pdf_path):
    try:
        # Open the PDF document
        doc = fitz.open(pdf_path)
        text = ''
        for page in doc:
            text += page.get_text()
        return text
    except Exception as e:
        print(f"Error processing file {pdf_path}: {e}")
        return None

def write_files(input_dir, output_path):
    # Check if the input directory exists
    if not os.path.isdir(input_dir):
        print(f"Input directory '{input_dir}' does not exist.")
        return

    # Ensure the output directory exists
    os.makedirs(output_path, exist_ok=True)

    for file in os.listdir(input_dir):
        input_file = os.path.join(input_dir, file)
        if os.path.isdir(input_file):
            print(f"skipping directory: {format(input_file)}")
            continue  # Skip directories

        pdf_text = pdf_to_text(input_file)
        if pdf_text is None:
            continue  # Skip if the PDF could not be read

        output_file = os.path.join(output_path, f"{os.path.splitext(file)[0]}.txt")
        with open(output_file, 'w') as f:
            f.write(pdf_text)

def get_cohere_api_key():
    """
    Retrieves the Cohere API key from an environment variable.
    """
    api_key = os.getenv('COHERE_API_KEY')
    if not api_key:
        raise EnvironmentError("COHERE_API_KEY environment variable not set.")
    return api_key


def process_syllabus(file_path):
    """
    Reads the syllabus text from the given file path.
    """
    with open(file_path, 'r') as file:
        syllabus_text = file.read()
    return syllabus_text

def extract_info(document_text, cohere_client):
    """
    Extracts tasks, task percentages, descriptions, and deadlines from the provided document text.
    """
    prompt = f"""
    Extract all tasks, task percentages, descriptions, and deadlines from the following document. 
    If any information (like description or deadline or task percentage) is missing, 
    write 'N/A' in that cell. Also, include any milestones or deliverables that are part of each task.
    Document: 
    {document_text}
    Format the output as follows:
    | Task | Task Percentage | Description | Deadline |
    | --- | --- | --- | --- |
    | <Task Name> | <Task Percentage> | <Description> | <Deadline> |
    """
    # Call the Cohere API to generate text
    response = cohere_client.generate(
        model='command-xlarge-nightly',  
        prompt=prompt,
        max_tokens=1500,
        temperature=0.7
    )

    # Extract and return the generated response text
    return response.generations[0].text

def extract_tasks(input_folder, output_folder, input_filename):
    # Get API key securely from environment variables
    load_dotenv()
    api_key = get_cohere_api_key()
    cohere_client = cohere.Client(api_key)

    # Create output folder if it does not exist
    os.makedirs(output_folder, exist_ok=True)

    # Process each file in the input folder
    for filename in os.listdir(input_folder):
        if filename != input_filename: 
            continue

        input_filepath = os.path.join(input_folder, filename)
        document_text = process_syllabus(input_filepath)

        output_text = extract_info(document_text, cohere_client)

        # Write the extracted information to an output file
        output_filepath = os.path.join(output_folder, f"{os.path.splitext(filename)[0]}_results.txt")
        with open(output_filepath, 'w') as result_file:
            result_file.write(output_text)


def extract_tasks_from_file(file_path, delimiter="|", save_path=None):
    """Extract tasks from a text or CSV file."""
    data = pd.read_csv(file_path, delimiter=delimiter)
    data.columns = data.columns.str.strip()  # Clean up column names
    data = data.applymap(lambda x: x.strip() if isinstance(x, str) else x)  # Clean data
    # Remove rows where the 'Task' column contains '---'
    data = data[data["Task"] != "---"]
    task_column = data["Task"]
    
    if save_path:
        task_column.to_csv(save_path, index=False, header=True)
        print(f"Cleaned task column saved to '{save_path}'.")

    return task_column.tolist()


def tokenize_function(examples):
    """Tokenize the text column."""
    tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
    return tokenizer(examples["tasks"], padding="max_length", truncation=True)


def classify_tasks(data_path, tasks, model):
        """Classify tasks using the trained RoBERTa model."""
        model.eval()
        tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        inputs = tokenizer(tasks, padding=True, truncation=True, max_length=128, return_tensors="pt")
        inputs = {key: value.to(device) for key, value in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=-1)

        df = pd.read_csv(data_path)
        df = df.drop(columns=['Unnamed: 0'], errors='ignore') #remove the first unnamed column
        df = df.drop(0).reset_index(drop=True) #remove first row
        df.columns = ['pdf', 'tasks', 'class']
        df['pdf'] = df['pdf'].fillna(method='ffill')
        
        print(df.head(10))
        print(df["class"])
        dataset = Dataset.from_pandas(df)
        dataset = dataset.class_encode_column("class")

        class_labels = dataset.features["class"].names
        predicted_labels = [class_labels[pred] for pred in predictions]

        return predicted_labels